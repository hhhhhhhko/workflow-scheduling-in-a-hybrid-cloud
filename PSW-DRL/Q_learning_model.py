# Q-learning algorithm

import numpy as np
import pandas as pd
import random
import matplotlib.pyplot as plt
import time
from preprocess.Workflow import Scientific_Workflow

'''
è¾“å…¥å·¥ä½œæµå±æ€§ï¼Œè¾“å‡ºå·¥ä½œæµä¸­ä»»åŠ¡çš„æ’åºï¼ˆè¦ç¬¦åˆçº¦æŸï¼‰
'''

class Q_learning():
    
    #def __init__(self, alpha, gamma, dag, Twait_list):
    def __init__(self, alpha, gamma, dag):
        '''
        alpha : å­¦ä¹ ç‡
        gamma : æŠ˜æ‰£å› å­
        dag : å·¥ä½œæµDAGå¯¹è±¡, å«æœ‰å·¥ä½œæµçš„å±æ€§num_of_vertexã€vertexã€edgeã€predã€succã€entryã€exitã€ranku
        q_sa : Q-table
        reward[i] : é€‰æ‹©ç¬¬iä¸ªä»»åŠ¡æ—¶çš„å³æ—¶å¥–åŠ±
        Twait_list
        '''
        self.alpha = alpha
        self.gamma = gamma
        self.dag = dag # å¼•å…¥XMLProcess.pyä¸­çš„XMLtoDAGç±»
        self.q_sa = [[0 for j in range(self.dag.n_job)] for i in range(self.dag.n_job)]
        self.reward = []
        self.eplsilon = 0.9 # greedy policy
        #self.reward_calc(Twait_list)
        self.reward_calc()

    # è·å¾—æœ€ä½³Q-table
    def learning(self):
        convergence_flag = 0 # åˆ¤æ–­æ˜¯å¦æ”¶æ•›
        episode = 0 # è®°å½•episodeæ¬¡æ•°
        return_1 = []
        q_table = [[0 for j in range(self.dag.n_job)] for i in range(self.dag.n_job)] # åˆå§‹åŒ–Q-table
        total_reward = 0 # è®°å½•æ€»å¥–åŠ±
        while(True):
            #total_reward = 0 # è®°å½•æ€»å¥–åŠ±
            '''
            current_state : å½“å‰çŠ¶æ€
            finish_vertexs : å·²é€‰å®šçš„ä»»åŠ¡
            wait_vertexs : å¯ä¾›é€‰æ‹©çš„ä»»åŠ¡
            selected_vertex : é€‰æ‹©çš„ä»»åŠ¡
            before_state : è½¬æ¢å‰çš„çŠ¶æ€
            r : åŠ¨ä½œè·å¾—çš„å³æ—¶å¥–åŠ±
            max_q_value : æœ€å¤§Qå€¼
            max_value_action : ä»è¿‡æ¸¡åçŠ¶æ€çš„è§’åº¦çœ‹ï¼Œå…·æœ‰æœ€å¤§åŠ¨ä½œä»·å€¼çš„åŠ¨ä½œ
            virtual_entry_index : è™šæ‹Ÿå…¥å£ä»»åŠ¡çš„ä»»åŠ¡å·
            '''
            
            if episode == 0:
                self.dag.add_virtual_entry()  # æ·»åŠ è™šæ‹Ÿå…¥å£ä»»åŠ¡
                self.dag.add_virtual_exit()  # æ·»åŠ è™šæ‹Ÿå‡ºå£ä»»åŠ¡
            current_state = -1
            # current_state = self.dag.virtual_entry_index  # åˆå§‹çŠ¶æ€æ˜¯è™šæ‹Ÿå…¥å£ä»»åŠ¡
            finish_vertexs = [-1]
            #finish_vertexs = [self.dag.virtual_entry_index()]  # è™šæ‹Ÿå…¥å£ä»»åŠ¡è¢«é€‰ä¸­
            wait_vertexs = []
            wait_vertexs += self.check_succ(current_state, finish_vertexs, wait_vertexs)  # ä½¿ç”¨def check_succ()å‡½æ•°æ£€æŸ¥
            '''
            wait_vertexs = wait_vertexs + self.check_succ(current_state, finish_vertexs, wait_vertexs)
            +è¡¨ç¤ºè¿æ¥ä¸¤ä¸ªåˆ—è¡¨ï¼Œè€Œä¸æ˜¯åŠ æ³•è¿ç®—
            '''
            no_update_flag = 0  #
            
            for k in range(self.dag.n_job - 1):
                
                '''
                if np.random.uniform() > self.eplsilon:  # ä»¥epsilonçš„æ¦‚ç‡é€‰æ‹©åŠ¨ä½œ
                    selected_vertex = random.choice(wait_vertexs)  # éšæœºå†³å®šåŠ¨ä½œ
                else:
                    if current_state == -1:
                        #state_actions = 0
                        selected_vertex = random.choice(wait_vertexs)
                    else:
                        state_actions = q_table[current_state]
                        max_legal_value = max (state_actions[i] for i in wait_vertexs)
                        if max_legal_value == 0:
                            selected_vertex = random.choice(wait_vertexs)
                        else:
                            selected_vertex = state_actions.index(max_legal_value)
                '''
                        
                        
                        # state_actions = q_table[current_state]  # è·å¾—å½“å‰çŠ¶æ€ä¸‹çš„æ‰€æœ‰åŠ¨ä½œ,
                        # legal_vertex = [state_actions[i] for i in wait_vertexs]  # è·å¾—å½“å‰çŠ¶æ€ä¸‹çš„æ‰€æœ‰åˆæ³•åŠ¨ä½œçš„qå€¼
                        # max_value_action = np.argmax(legal_vertex)  # è¿”å›å¯å–åˆæ³•ç´¢å¼•å¯¹åº”å€¼çš„ç´¢å¼•å·
                        # selected_vertex = np.where(legal_vertex == max_value_action)[0][0]  # é€‰æ‹©å…·æœ‰æœ€å¤§Qå€¼çš„åˆæ³•çš„åŠ¨ä½œ,å³æœ€å¤§ç´¢å¼•å·åœ¨state_actionsä¸­çš„ç´¢å¼•å·
                        # selected_vertex = state_actions.index(max_value_action)  ğŸ‘ˆ0ä¼šé‡å¤,æ‰€ä»¥ä¸å‡†ç¡®
                        
                selected_vertex = random.choice(wait_vertexs)
                
                wait_vertexs.remove(selected_vertex)  # ä»å¯ä¾›é€‰æ‹©çš„ä»»åŠ¡ä¸­åˆ é™¤è¢«é€‰èµ°çš„ä»»åŠ¡
                finish_vertexs.append(selected_vertex)  # å°†è¢«é€‰ä¸­çš„ä»»åŠ¡åŠ å…¥å·²é€‰å®šçš„ä»»åŠ¡ä¸­
                before_state = current_state  # è®°å½•è½¬æ¢å‰çš„çŠ¶æ€
                current_state = selected_vertex  # è½¬æ¢çŠ¶æ€
                r = self.reward[selected_vertex]  # è·å¾—å³æ—¶å¥–åŠ±
                wait_vertexs += self.check_succ(current_state, finish_vertexs, wait_vertexs)  # ä½¿ç”¨def check_succ()å‡½æ•°æ£€æŸ¥

                max_q_value = 0
                max_value_action = 0

                for n in range(self.dag.n_job):  # æ›´æ–°æœ€å¤§Qå€¼
                    if(q_table[current_state][n] >= max_q_value):  
                        max_q_value = q_table[current_state][n]
                        max_value_action = n

                # æ›´æ–°Q-table
                before_q_sa = q_table[before_state][selected_vertex]  # è®°å½•æ›´æ–°å‰çš„Q-table
                q_table[before_state][selected_vertex] = q_table[before_state][selected_vertex] + self.alpha * (r + self.gamma * max_q_value - q_table[before_state][selected_vertex])
                
                # è®°å½•return(ä¸€ä¸ªepisodeä¸­çš„æ€»å¥–åŠ±)
                total_reward += self.alpha * (r + self.gamma * max_q_value - q_table[before_state][selected_vertex])
                #total_reward += r
                

                if(abs(q_table[before_state][selected_vertex] - before_q_sa) <= 1):  # å¦‚æœæ›´æ–°é‡å°äº1
                    no_update_flag+=1

            return_1.append(total_reward)

            #return finish_vertexs
            # å¾ªç¯ç»“æŸçš„åˆ¤å®šæ¡ä»¶
            # if(no_update_flag == self.dag.num_of_vertex * (self.dag.num_of_vertex - 1)):  # å¦‚æœQ-tableåœ¨ä¸€è½®ä¸­æ²¡æœ‰æ›´æ–°
            episode += 1
            if(no_update_flag == (self.dag.n_job - 1)):    
                convergence_flag+=1
                if(convergence_flag == 100000):  
                    # 1000ä¸ªä»»åŠ¡1000000æœ‰ç‚¹å¤§
                    #100ä¸ªä»»åŠ¡éœ€è¦å¤§çº¦500000/600000ã€‚70éœ€è¦200000ã€‚50éœ€è¦100000/200000, 30éœ€è¦10000/20000ã€‚ çœ‹æƒ…å†µè°ƒæ•´
                    break
            
        return finish_vertexs, q_table, episode, return_1

    # è¿”å›ä¸€ä¸ª "nä¸ªåç»­ä»»åŠ¡åˆæ³•ä¸”ä¸åœ¨wait_vertexsä¸­ "çš„ä»»åŠ¡çš„åˆ—è¡¨
    def check_succ(self, n, finish_vertexs, wait_vertexs):
        list = []
        
        #for succ_n in range(len(self.dag.successor)):
        for succ_n in self.dag.successor[n+1]:
            if(self.legal(succ_n, finish_vertexs) and succ_n not in wait_vertexs):
                list.append(succ_n)
        
        return list
    
    # åˆ¤æ–­ä»»åŠ¡næ˜¯å¦åˆæ³•
    def legal(self, n, finish_vertexs):
        for pred_n in self.dag.precursor[n+1]:
        #for pred_n in range(len(self.dag.precursor[n])):
            if(pred_n not in finish_vertexs):
                return False
        
        return True
    
    # å¥–åŠ±çš„ç¡®å®š(è®¾ç½®rankuä¸ºreward)
    #def reward_calc(self, Twait_list):
    def reward_calc(self):
        #scientific_workflow = Scientific_Workflow('CyberShake', 100)
        #dag = scientific_workflow.get_workflow()
        #ranku = dag.get_ranku()
        ranku = self.dag.get_ranku()
        for i in range(self.dag.n_job):
        #for i in range(dag.n_job):
            #self.reward.append(ranku[i] + 0.001 * Twait_list[i])
            #self.reward.append(self.dag.ranku[i])
            self.reward.append(ranku[i])

    # è¡¨ç¤ºq_saï¼ˆè½¬æ¢ä¸ºæ•´æ•°å½¢å¼ï¼‰
    def print_q_sa_int(self):
        q_sa_int = [[0 for j in range(self.dag.n_job)] for i in range(self.dag.n_job)]
        
        for i in range(self.dag.n_job):
            for j in range(self.dag.n_job):
                q_sa_int[i][j] = int(self.q_sa[i][j])
        
        print("q_sa_int = ", end = "")
        for i in range(self.dag.n_job):
            print(q_sa_int[i])